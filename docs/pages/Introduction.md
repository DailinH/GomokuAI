## Current Score

### Week 3

* 目前，我们将注意力集中于结构与性能的优化，因此暂时不急于参与至天梯中。
* 因此Botzone分数为：1000(〜￣△￣)〜。
* 不过，即使是目前的仍然有潜在Bug的只会横下五子的AI，也能打赢绝大部分的随机AI了。
* 此外，不知为何，目前的Bot代码能在Windows下正常运行，但一旦在Botzone的Linux环境下跑，就会出现Runtime Error_ (:з」∠ )_


### Week 5

* 本期仍为开发的过渡期，主要工作是建立起来了项目的完整结构与模块间的功能依赖关系，且明确了要做一个神经网络版AI的目标。
* 目前，AlphaZero版AI的开发接近完成，等待着实地的训练与测试。当它能够稳定的变强以后，我们便有了一个稳靠的火器，届时也将正式投入与他人的竞技之中。


## Idea of Design

我们的目标是~~：没有蛀牙！~~：

- High Performance
- High Readablity
- High Scalbility

在可读性与扩展性上，我们采用工程化手段进行AI的开发，使用Python作为胶水语言；

在性能上，我们采用C++编写AI中性能关键的模块，并包装成为Python模块的扩展。

## Expectations

简单来说，它应能够达成如下的效果：

1. 在性能上达到一个极致的平衡，没有浪费的操作。
2. 可以方便地在不同的算法之间切换。
3. 可以提供一个方便的与Botzone交互的接口。

（待更新）

## Further Plans & Directions

### Week 3

* 在开始写报告后，我们为MCTS设计了`Policy`这个重要的结构。有了它，我们可以方便地对MCTS的关键算法进行替换，由此可以对各种不同算法的性能进行改进与评估。因此，未来的报告中，或许可以方便地列出各种baseline算法相互比较的图表。

* 起初，我们组的一个大方向是「使用C++内核 + Python搭建神经网络来实现AI」。现在，这个方向反而变得不那么明晰了。因为我们不太清楚不使用神经网络，且只用纯C++代码到底能将AI性能推进到什么样的程度。

  我们将在实现一些有名的MCTS优化算法（如：RAVE算法）后，根据AI的表现来决定是否执行最初的路线。

### Week 5

- 在得知4班的某位同学成功跑起来了AlphaZero版AI后，我们组终于定下心来决定跟进。本来是打算在最后一期做出来作为收尾之笔的，但经过仔细考虑后，提早做出来有不少的好处：

  - 考虑到神经网络需要较长的训练时间，早早做出来，显然为之后的优化与等待留有较大的余地。
  - 神经网络是一个在开发过程中，你能持续明显感觉到「它在变强」的东西。因此，它可以成为我们的基线模型，在训练过程中产生的各种checkpoint也可以用以与后续的其他模型比较。

- 在AlphaZero版AI能够顺畅地运行以后，我们将继续研究传统方法，以作为未来的主要基线方法，及备选参赛选手。不过，我们不打算再分别实现很多种不同算法了。我们将维护一个单独的「TraditionalAgent」，使用各种算法对其集中优化，以用来和AlphaZeroAgent进行有意义的比较。

- 未来的主要优化方向：

  - 算法上：

    由于AlphaZero需要大量的计算力来达到较好的训练速度，（例如论文中使用了5000个TPU v1来生成自对弈棋谱），而我们显然无法负担起如此庞大的算力，因此需要使用一些手段来对其进行优化。

    未来一期将采取的优化手段为：

    - [Decisive and Anti-Decisive Moves](https://hal.inria.fr/inria-00495078/document)。它以一个简单的思想利用领域知识来强化Select与Simulate过程。这种优化的特点是它不会对AI的自我学习造成负面影响，仅仅是加速了AI的收敛速度：因为其优化点在某种程度上是AI必须抵达的终点。
    - 设计卷积核的初始权重，以期加速收敛。

  - 工程上：

    - Memory Pool。经过Profiler测试显示，在目前的默认策略与MCTS配置下，有40%~50%的时间开销花在了expand阶段，并且几乎所有时间开销都集中在`operator new`，也就是树结点的内存分配上。MCTS的记录显示，一轮Playout后，结点的个数高达$4×10^7$个。

      写一个高性能的内存池，应该能够对此性能瓶颈有较好的改善。

    - Cache。目前每次backPropogate操作都会将棋盘回退到开始局面，再进行下一轮Playout。但是，当Exploitation的值足够大时，很有可能下一轮Playout大部分仍走原来的路径。如果要加上盘面检查机制的话，这种重复操作的开销可能是很大的。

      我们可以添加一个简单的Cache机制，即每轮Playout后Board暂时保持叶结点的局面，在下一轮Playout中，一直到Select阶段选到的结点不在上一轮路径为止，才对Board进行回退操作。